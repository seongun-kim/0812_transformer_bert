{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "bert",
      "language": "python",
      "name": "samsung_bert"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fpQaKA4cYHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from bert_util import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYNujmDVcYH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertConfig():\n",
        "    r\"\"\"\n",
        "        This is the configuration class to store the configuration of a :class:`~transformers.BertModel`.\n",
        "        It is used to instantiate an BERT model according to the specified arguments, defining the model\n",
        "        architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of\n",
        "        the BERT `bert-base-uncased <https://huggingface.co/bert-base-uncased>`__ architecture.\n",
        "        Configuration objects inherit from  :class:`~transformers.PretrainedConfig` and can be used\n",
        "        to control the model outputs. Read the documentation from  :class:`~transformers.PretrainedConfig`\n",
        "        for more information.\n",
        "        Args:\n",
        "            vocab_size (:obj:`int`, optional, defaults to 30522):\n",
        "                Vocabulary size of the BERT model. Defines the different tokens that\n",
        "                can be represented by the `inputs_ids` passed to the forward method of :class:`~transformers.BertModel`.\n",
        "            hidden_size (:obj:`int`, optional, defaults to 768):\n",
        "                Dimensionality of the encoder layers and the pooler layer.\n",
        "            num_hidden_layers (:obj:`int`, optional, defaults to 12):\n",
        "                Number of hidden layers in the Transformer encoder.\n",
        "            num_attention_heads (:obj:`int`, optional, defaults to 12):\n",
        "                Number of attention heads for each attention layer in the Transformer encoder.\n",
        "            intermediate_size (:obj:`int`, optional, defaults to 3072):\n",
        "                Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n",
        "            hidden_act (:obj:`str` or :obj:`function`, optional, defaults to \"gelu\"):\n",
        "                The non-linear activation function (function or string) in the encoder and pooler.\n",
        "                If string, \"gelu\", \"relu\", \"swish\" and \"gelu_new\" are supported.\n",
        "            hidden_dropout_prob (:obj:`float`, optional, defaults to 0.1):\n",
        "                The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.\n",
        "            attention_probs_dropout_prob (:obj:`float`, optional, defaults to 0.1):\n",
        "                The dropout ratio for the attention probabilities.\n",
        "            max_position_embeddings (:obj:`int`, optional, defaults to 512):\n",
        "                The maximum sequence length that this model might ever be used with.\n",
        "                Typically set this to something large just in case (e.g., 512 or 1024 or 2048).\n",
        "            type_vocab_size (:obj:`int`, optional, defaults to 2):\n",
        "                The vocabulary size of the `token_type_ids` passed into :class:`~transformers.BertModel`.\n",
        "            initializer_range (:obj:`float`, optional, defaults to 0.02):\n",
        "                The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
        "            layer_norm_eps (:obj:`float`, optional, defaults to 1e-12):\n",
        "                The epsilon used by the layer normalization layers.\n",
        "            gradient_checkpointing (:obj:`bool`, optional, defaults to False):\n",
        "                If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
        "        Example::\n",
        "            >>> from transformers import BertModel, BertConfig\n",
        "            >>> # Initializing a BERT bert-base-uncased style configuration\n",
        "            >>> configuration = BertConfig()\n",
        "            >>> # Initializing a model from the bert-base-uncased style configuration\n",
        "            >>> model = BertModel(configuration)\n",
        "            >>> # Accessing the model configuration\n",
        "            >>> configuration = model.config\n",
        "    \"\"\"\n",
        "    model_type = \"bert\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=30522,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=12,\n",
        "        num_attention_heads=12,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=gelu,\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        max_position_embeddings=512,\n",
        "        type_vocab_size=2,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        pad_token_id=0,\n",
        "        gradient_checkpointing=False,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        #super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.gradient_checkpointing = gradient_checkpointing\n",
        "        \n",
        "        self.output_attentions = output_attentions\n",
        "        self.output_hidden_states = output_hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZCIp285cYH4",
        "colab_type": "text"
      },
      "source": [
        "# BERT structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGYEX5yVcYH5",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"images/bert1.png\" width=\"700\" alt=\"bert_structure\">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_94Y_1SXcYH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TFBertEmbeddings(tf.keras.layers.Layer):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.initializer_range = config.initializer_range\n",
        "        self.position_embeddings = tf.keras.layers.Embedding(\n",
        "            config.max_position_embeddings,\n",
        "            config.hidden_size,\n",
        "            embeddings_initializer=get_initializer(self.initializer_range),\n",
        "            name=\"position_embeddings\",\n",
        "        )\n",
        "        self.token_type_embeddings = tf.keras.layers.Embedding(\n",
        "            config.type_vocab_size,\n",
        "            config.hidden_size,\n",
        "            embeddings_initializer=get_initializer(self.initializer_range),\n",
        "            name=\"token_type_embeddings\",\n",
        "        )\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Build shared word embedding layer \"\"\"\n",
        "        with tf.name_scope(\"word_embeddings\"):\n",
        "            # Create and initialize weights. The random normal initializer was chosen\n",
        "            # arbitrarily, and works well.\n",
        "            self.word_embeddings = self.add_weight(\n",
        "                \"weight\",\n",
        "                shape=[self.vocab_size, self.hidden_size],\n",
        "                initializer=get_initializer(self.initializer_range),\n",
        "            )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        position_ids=None,\n",
        "        token_type_ids=None,\n",
        "        inputs_embeds=None,\n",
        "        mode=\"embedding\",\n",
        "        training=False,\n",
        "    ):\n",
        "        \"\"\"Get token embeddings of inputs.\n",
        "        Args:\n",
        "            inputs: list of three int64 tensors with shape [batch_size, length]: (input_ids, position_ids, token_type_ids)\n",
        "            mode: string, a valid value is one of \"embedding\" and \"linear\".\n",
        "        Returns:\n",
        "            outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\n",
        "                shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\n",
        "                linear tensor, float32 with shape [batch_size, length, vocab_size].\n",
        "        Raises:\n",
        "            ValueError: if mode is not valid.\n",
        "        Shared weights logic adapted from\n",
        "            https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24\n",
        "        \"\"\"\n",
        "        if mode == \"embedding\":\n",
        "            return self._embedding(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n",
        "        elif mode == \"linear\":\n",
        "            return self._linear(input_ids)\n",
        "        else:\n",
        "            raise ValueError(\"mode {} is not valid.\".format(mode))\n",
        "\n",
        "    def _embedding(self, input_ids, position_ids, token_type_ids, inputs_embeds, training=False):\n",
        "        \"\"\"Applies embedding based on inputs tensor.\"\"\"\n",
        "        assert not (input_ids is None and inputs_embeds is None)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            input_shape = shape_list(input_ids)\n",
        "        else:\n",
        "            input_shape = shape_list(inputs_embeds)[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = tf.fill(input_shape, 0)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\n",
        "\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings, training=training)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def _linear(self, inputs):\n",
        "        \"\"\"Computes logits by running inputs through a linear layer.\n",
        "            Args:\n",
        "                inputs: A float32 tensor with shape [batch_size, length, hidden_size]\n",
        "            Returns:\n",
        "                float32 tensor with shape [batch_size, length, vocab_size].\n",
        "        \"\"\"\n",
        "        batch_size = shape_list(inputs)[0]\n",
        "        length = shape_list(inputs)[1]\n",
        "        x = tf.reshape(inputs, [-1, self.hidden_size])\n",
        "        logits = tf.matmul(x, self.word_embeddings, transpose_b=True)\n",
        "\n",
        "        return tf.reshape(logits, [batch_size, length, self.vocab_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0RHeT4kcYH7",
        "colab_type": "text"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvfXowAFcYH7",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
        "\n",
        "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnj6CAwocYH8",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"images/multi_head_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8ZBa4U5cYH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TFBertSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        assert config.hidden_size % config.num_attention_heads == 0\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        self.query = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n",
        "        )\n",
        "        self.key = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n",
        "        )\n",
        "        self.value = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n",
        "        )\n",
        "        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n",
        "\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, hidden_states, attention_mask, output_attentions, training=False):\n",
        "\n",
        "        batch_size = shape_list(hidden_states)[0]\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = tf.matmul(\n",
        "            query_layer, key_layer, transpose_b=True\n",
        "        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n",
        "        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)  # scale attention_scores\n",
        "        attention_scores = attention_scores / tf.math.sqrt(dk)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs, training=training)\n",
        "\n",
        "        context_layer = tf.matmul(attention_probs, value_layer)\n",
        "        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n",
        "        context_layer = tf.reshape(\n",
        "            context_layer, (batch_size, -1, self.all_head_size)\n",
        "        )  # (batch_size, seq_len_q, all_head_size)\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLNhM00-cYH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TFBertIntermediate(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
        "        )\n",
        "\n",
        "        self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcVWnN2acYIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TFBertOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def call(self, hidden_states, input_tensor, training=False):\n",
        "        print(\"Hidden state has shape of :\" , hidden_states.shape)\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states, training=training)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class TFTransformer(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.self_attention = TFBertSelfAttention(config, name=\"attention\")\n",
        "        self.intermediate = TFBertIntermediate(config, name=\"intermediate\")\n",
        "        self.dropNnorm1 = TFBertOutput(config, name=\"output1\")\n",
        "        self.dropNnorm2 = TFBertOutput(config, name=\"output2\")\n",
        "\n",
        "\n",
        "    def call(self, input_tensor, attention_mask, output_attentions, training=False):\n",
        "        print(\"Hey\", output_attentions)\n",
        "        self_outputs = self.self_attention(\n",
        "            input_tensor, attention_mask, output_attentions, training=training\n",
        "        )\n",
        "        attention_output = self.dropNnorm1(self_outputs[0], input_tensor, training=training)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        print(\"intermediate output: \", intermediate_output.shape)\n",
        "        layer_output = self.dropNnorm2(intermediate_output, attention_output, training=training)\n",
        "        outputs = (layer_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "\n",
        "        return outputs\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6WNM2SAcYIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TFBertPooler(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            config.hidden_size,\n",
        "            kernel_initializer=get_initializer(config.initializer_range),\n",
        "            activation=\"tanh\",\n",
        "            name=\"dense\",\n",
        "        )\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "\n",
        "        return pooled_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsJYisPQcYIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TFBertMLMHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, input_embeddings, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        self.transform_act_fn = config.hidden_act\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.input_embeddings = input_embeddings\n",
        "        \n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        hidden_states = self.input_embeddings(hidden_states, mode=\"linear\")\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class TFBertNSPHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pooler = TFBertPooler(config, name=\"pooler\")\n",
        "        self.seq_relationship = tf.keras.layers.Dense(\n",
        "            2, kernel_initializer=get_initializer(config.initializer_range), name=\"seq_relationship\"\n",
        "        )\n",
        "\n",
        "    def call(self, sequence_output):\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "\n",
        "        return seq_relationship_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKb9RBGWcYIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TFBertMainLayer(tf.keras.layers.Layer):\n",
        "    config_class = BertConfig\n",
        "\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_hidden_layers = config.num_hidden_layers\n",
        "        self.initializer_range = config.initializer_range\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.embeddings = TFBertEmbeddings(config, name=\"embeddings\")\n",
        "        self.encoder = TFTransformer(config, name=\"encoder\")\n",
        "        self.nsp = TFBertNSPHead(config, name=\"nsp___cls\")\n",
        "        self.mlm = TFBertMLMHead(config, self.embeddings, name=\"mlm___cls\")\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "        self.embeddings.vocab_size = value.shape[0]\n",
        "\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        training=False,\n",
        "    ):\n",
        "        input_ids = inputs\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = shape_list(input_ids)\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = shape_list(inputs_embeds)[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = tf.fill(input_shape, 1)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = tf.fill(input_shape, 0)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "\n",
        "        extended_attention_mask = tf.cast(extended_attention_mask, tf.float32)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            extended_attention_mask,\n",
        "            output_attentions,\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        prediction_scores = self.mlm(sequence_output)#, training=kwargs.get(\"training\", False))\n",
        "        seq_relationship_score = self.nsp(sequence_output)\n",
        "\n",
        "        outputs = (prediction_scores, seq_relationship_score,) + encoder_outputs[\n",
        "            1:\n",
        "        ]  # add hidden states and attention if they are here\n",
        "\n",
        "        \n",
        "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1jgpw1HcYII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = BertConfig()\n",
        "model = TFBertMainLayer(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LupGbncwcYIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_input = np.array([[  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
        "         2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
        "         3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
        "         1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
        "         2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0]])\n",
        "\n",
        "sample_mask1 = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "\n",
        "\n",
        "sample_mask2 = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "\n",
        "\n",
        "\n",
        "sample_label = np.array([1])\n",
        "\n",
        "sample = [sample_input,sample_mask2,sample_mask1,sample_label]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRi3-04rcYIL",
        "colab_type": "code",
        "colab": {},
        "outputId": "ad11aa0d-cd94-4db6-a8f5-05f760f6f033"
      },
      "source": [
        "output = model(sample_input, attention_mask=sample_mask2, token_type_ids=sample_mask1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hey False\n",
            "Hidden state has shape of : (1, 128, 768)\n",
            "intermediate output:  (1, 128, 3072)\n",
            "Hidden state has shape of : (1, 128, 3072)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Eu14NlicYIN",
        "colab_type": "code",
        "colab": {},
        "outputId": "2aa00d68-dce1-4c57-9e60-ad6c9823018b"
      },
      "source": [
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: id=499, shape=(1, 128, 30522), dtype=float32, numpy=\n",
              " array([[[-0.8636676 , -0.4042933 ,  0.83449423, ..., -0.4818025 ,\n",
              "           0.1139232 , -0.44945532],\n",
              "         [-0.21980987,  0.48217708, -0.53139687, ..., -0.18168671,\n",
              "           0.20353438, -0.562743  ],\n",
              "         [-1.3677536 ,  0.2972694 , -0.4992544 , ...,  0.38428855,\n",
              "           0.36869404, -0.49694228],\n",
              "         ...,\n",
              "         [-0.78096783, -0.501459  , -0.03552767, ...,  0.06676748,\n",
              "           0.2258226 , -0.26428467],\n",
              "         [-0.34884116, -0.77756035, -0.0318495 , ...,  0.61443055,\n",
              "          -0.17418608,  0.38312015],\n",
              "         [-0.8351403 , -1.08515   ,  0.18950324, ...,  0.4293853 ,\n",
              "           0.07094439,  0.15951923]]], dtype=float32)>,\n",
              " <tf.Tensor: id=554, shape=(1, 2), dtype=float32, numpy=array([[0.24967924, 0.19858795]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XDG_8OxbyCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_MLM_loss(labels, mask, logits):\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
        "    )\n",
        "    # make sure only labels that are not equal to -100\n",
        "    # are taken into account as loss\n",
        "    #active_loss = tf.reshape(labels, (-1,)) != -100\n",
        "    \n",
        "    #reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), tf.reshape(mask, (-1,)))\n",
        "    #labels = tf.boolean_mask(tf.reshape(labels, (-1,)), tf.reshape(mask, (-1,)))\n",
        "    reduced_logits = tf.reshape(logits, (-1, shape_list(logits)[2]))\n",
        "    labels = tf.reshape(labels, (-1,))\n",
        "    \n",
        "    loss = loss_fn(labels, reduced_logits)\n",
        "    loss = tf.reduce_sum(loss) / loss.shape[0]\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LId1qbbkbyCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_NSP_loss(labels, logits):\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    labels = tf.reshape(labels, [-1])\n",
        "    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAqZKDNScYIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NSP_loss = compute_NSP_loss(labels=sample_label, logits=output[1])\n",
        "MLM_loss = compute_MLM_loss(sample_input, output[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}