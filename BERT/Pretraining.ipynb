{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "bert",
      "language": "python",
      "name": "samsung_bert"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Pretraining (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MC3-PbCataw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "a35ee520-1c6e-41ab-f22d-7ec8b60d8056"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import csv\n",
        "import random\n",
        "\n",
        "from bert_util import *\n",
        "%run ./BERT.ipynb"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hey False\n",
            "Hidden state has shape of : (1, 128, 768)\n",
            "intermediate output:  (1, 128, 3072)\n",
            "Hidden state has shape of : (1, 128, 3072)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWe4zPu6ataz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file = open('data/imdb_train.csv')\n",
        "\n",
        "csv_reader = csv.reader(data_file)\n",
        "\n",
        "dataset = []\n",
        "for line in csv_reader:\n",
        "    dataset.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haqUW-7Hata1",
        "colab_type": "code",
        "colab": {},
        "outputId": "aeb6eeb5-d258-4f21-c3bf-449a50074b04"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['positive',\n",
              " \"one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked.\",\n",
              " 'they are right, as this is exactly what happened with me.',\n",
              " 'the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go.',\n",
              " 'trust me, this is not a show for the faint hearted or timid.',\n",
              " 'this show pulls no punches with regards to drugs, sex or violence.',\n",
              " 'its is hardcore, in the classic use of the word.',\n",
              " 'it is called oz as that is the nickname given to the oswald maximum security state penitentary.',\n",
              " 'it focuses mainly on emerald city, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda.',\n",
              " 'em city is home to many..aryans, muslims, gangstas, latinos, christians, italians, irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.',\n",
              " \"i would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare.\",\n",
              " \"forget pretty pictures painted for mainstream audiences, forget charm, forget romance...oz doesn't mess around.\",\n",
              " \"the first episode i ever saw struck me as so nasty it was surreal, i couldn't say i was ready for it, but as i watched more, i developed a taste for oz, and got accustomed to the high levels of graphic violence.\",\n",
              " \"not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) watching oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtIpzOG2ata4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 첫번째에 있는 sentiment label은 pretraining에서 사용하지 않으므로 제거합니다.\n",
        "\n",
        "dataset = [data[1:] for data in dataset]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ3uh-0Fata6",
        "colab_type": "code",
        "colab": {},
        "outputId": "263f9a95-fceb-4e0b-9605-a8a37603dcfe"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked.\",\n",
              " 'they are right, as this is exactly what happened with me.',\n",
              " 'the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go.',\n",
              " 'trust me, this is not a show for the faint hearted or timid.',\n",
              " 'this show pulls no punches with regards to drugs, sex or violence.',\n",
              " 'its is hardcore, in the classic use of the word.',\n",
              " 'it is called oz as that is the nickname given to the oswald maximum security state penitentary.',\n",
              " 'it focuses mainly on emerald city, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda.',\n",
              " 'em city is home to many..aryans, muslims, gangstas, latinos, christians, italians, irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.',\n",
              " \"i would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare.\",\n",
              " \"forget pretty pictures painted for mainstream audiences, forget charm, forget romance...oz doesn't mess around.\",\n",
              " \"the first episode i ever saw struck me as so nasty it was surreal, i couldn't say i was ready for it, but as i watched more, i developed a taste for oz, and got accustomed to the high levels of graphic violence.\",\n",
              " \"not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) watching oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHC7xP4Wata8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_file = open('vocab.txt') # pretrained model에서 사전에 정의된 vocabulary 그대로 사용\n",
        "\n",
        "vocab = vocab_file.readlines()\n",
        "vocab = [word.strip() for word in vocab]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw2YA3DOata-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2id = {vocab[idx]:idx for idx in range(len(vocab)) }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFFBx015atbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wp = WordpieceTokenizer(vocab, '[UNK]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ZdXnIdatbB",
        "colab_type": "code",
        "colab": {},
        "outputId": "1530b43d-c16e-4a8d-8bfd-fedd9bdee039"
      },
      "source": [
        "tokenized_dataset = []\n",
        "\n",
        "\n",
        "for i in trange(len(dataset[:1000])):\n",
        "    tokenized = [wp.tokenize(sent) for sent in dataset[i]]\n",
        "    tokenized_dataset.append(tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:22<00:00, 12.07it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS4gQDfGatbD",
        "colab_type": "code",
        "colab": {},
        "outputId": "c157703d-45de-444f-aaa4-e553969227b7"
      },
      "source": [
        "tokenized_dataset[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'other',\n",
              " 'reviewers',\n",
              " 'has',\n",
              " 'mentioned',\n",
              " 'that',\n",
              " 'after',\n",
              " 'watching',\n",
              " 'just',\n",
              " '1',\n",
              " 'oz',\n",
              " 'episode',\n",
              " 'you',\n",
              " \"##'\",\n",
              " '##ll',\n",
              " 'be',\n",
              " 'hooked',\n",
              " '##.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYWxhQxpatbF",
        "colab_type": "code",
        "colab": {},
        "outputId": "fdd048c8-29f3-41da-ae21-6433a6e091b6"
      },
      "source": [
        "dataset[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_2mY2f_atbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indexed_dataset = [[[word2id[word] for word in sent] for sent in par] for par in tokenized_dataset]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZBbEVxKatbI",
        "colab_type": "code",
        "colab": {},
        "outputId": "a3303820-20cb-44ef-9698-c61911c91201"
      },
      "source": [
        "indexed_dataset[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2028,\n",
              " 1997,\n",
              " 1996,\n",
              " 2060,\n",
              " 15814,\n",
              " 2038,\n",
              " 3855,\n",
              " 2008,\n",
              " 2044,\n",
              " 3666,\n",
              " 2074,\n",
              " 1015,\n",
              " 11472,\n",
              " 2792,\n",
              " 2017,\n",
              " 29618,\n",
              " 3363,\n",
              " 2022,\n",
              " 13322,\n",
              " 29625]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bTACm-XatbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PretrainDataset():\n",
        "    def __init__(self, dataset, vocab):\n",
        "        \"\"\" Maked Language Modeling & Next Sentence Prediction dataset initializer\n",
        "        Use below attributes when implementing the dataset\n",
        "\n",
        "        Attributes:\n",
        "        dataset -- Paragraph dataset to make a MLM & NSP sample\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.vocab = vocab\n",
        "        self.CLS = word2id['[CLS]']\n",
        "        self.SEP = word2id['[SEP]']\n",
        "        self.MSK = word2id['[MASK]']\n",
        "        self.PARA_NUM = len(self.dataset)\n",
        "        self.par_len = [len(par) for par in dataset]\n",
        "        self.max_len = 128\n",
        "\n",
        "        \n",
        "        #self.special_tokens = [CLS, SEP, MSK]\n",
        "    @property\n",
        "    def token_num(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "    \n",
        "    def masking(self, sen1, sen2):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        sen1 -- 인덱스로 된 첫번째 문장, List(int)\n",
        "        sen2 -- 인덱스로 된 두번째 문장, List(int)\n",
        "        \n",
        "        Output:\n",
        "        MLM_sentences -- sen1+sen2의 전체 시퀀스에서 15%의 token을 선택하여, 그 중 80%는 MSK token으로 대체하고,\n",
        "                         10%는 랜덤 token으로 대체, 나머지 10%는 원래 token을 그대로 사용.\n",
        "                         len(MLM_sentences) = len(sen1)+len(sen2)+3 \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        MLM_sentences = sen1+sen2\n",
        "        TOKEN_NUM = self.token_num\n",
        "        \n",
        "        masking_id = random.sample(range(len(MLM_sentences)), int(len(MLM_sentences)*0.15))\n",
        "        random.shuffle(masking_id)\n",
        "        \n",
        "        mask = [True if i in masking_id else False for i in range(len(MLM_sentences))]\n",
        "        MLM_mask = [False] + mask[:len(sen1)] + [False] +  mask[len(sen1):] + [False]\n",
        "\n",
        "        for i in range(len(masking_id)):\n",
        "            if i < len(masking_id)*0.8:\n",
        "                MLM_sentences[masking_id[i]] = self.MSK\n",
        "            elif 0.8*len(masking_id) <= i and i < 0.9*len(masking_id):\n",
        "                rand_id = random.randrange(TOKEN_NUM-4)\n",
        "                spc_tks = [self.CLS, self.SEP, self.MSK, MLM_sentences[masking_id[i]]]                               \n",
        "                rand_id = rand_id if rand_id not in spc_tks else TOKEN_NUM - spc_tks.index(rand_id)-1\n",
        "                MLM_sentences[masking_id[i]] = rand_id\n",
        "        MLM_sentences = [self.CLS] + MLM_sentences[:len(sen1)] + [self.SEP] + MLM_sentences[len(sen1):] + [self.SEP]\n",
        "\n",
        "        return MLM_sentences, MLM_mask\n",
        "    \n",
        "    def positive_sampling(self):\n",
        "        \n",
        "        valid_par = np.where(np.array(self.par_len)>1)[0]\n",
        "                             \n",
        "        par_id1 = random.choice(valid_par)#(PARA_NUM)\n",
        "        par1 = self.dataset[par_id1]\n",
        "                             \n",
        "        sen_id1 = random.randrange(len(par1)-1)#randrange(len(par1)-1)\n",
        "                             \n",
        "        sen1 = par1[sen_id1]\n",
        "        sen2 = par1[sen_id1+1]\n",
        "        \n",
        "        return sen1, sen2\n",
        "    \n",
        "    def negative_sampling(self):\n",
        "        \n",
        "        par_id1, par_id2 = random.sample(range(self.PARA_NUM), 2)\n",
        "                             \n",
        "        par1 = self.dataset[par_id1]\n",
        "        par2 = self.dataset[par_id2]\n",
        "                             \n",
        "        sen_id1 = random.randrange(len(par1))\n",
        "        sen_id2 = random.randrange(len(par2))\n",
        "\n",
        "        sen1 = par1[sen_id1]\n",
        "        sen2 = par2[sen_id2]\n",
        "        \n",
        "        return sen1, sen2\n",
        "                \n",
        "    \n",
        "    def __iter__(self):\n",
        "        \"\"\" Masked Language Modeling & Next Sentence Prediction dataset\n",
        "        Sample two sentences from the dataset, and make a self-supervised pretraining sample for MLM & NSP\n",
        "\n",
        "        Note: You can use any sampling method you know.\n",
        "\n",
        "        Yields:\n",
        "        source_sentences: List[int] -- Sampled sentences\n",
        "        MLM_sentences: List[int] -- Masked sentences\n",
        "        MLM_mask: List[bool] -- Masking for MLM\n",
        "        NSP_label: bool -- NSP label which indicates whether the sentences is connected.\n",
        "\n",
        "        Example: If 25% mask with 50 % <msk> + 25% random + 25% same -- this percentage is just a example.\n",
        "        source_sentences = ['<cls>', 'He', 'bought', 'a', 'gallon', 'of', 'milk',\n",
        "                            '<sep>', 'He', 'drank', 'it', 'all', 'on', 'the', 'spot', '<sep>']\n",
        "        MLM_sentences = ['<cls>', 'He', '<msk>', 'a', 'gallon', 'of, 'milk',\n",
        "                         '<sep>', 'He', 'drank', 'it', 'tree', 'on', '<msk>', 'spot', '<sep>']\n",
        "        MLM_mask = [False, False, True, False, False, False, False,\n",
        "                    False, True, False, False, True, False True, False, False]\n",
        "        NSP_label = True\n",
        "        \"\"\"\n",
        "        \n",
        "        while True:\n",
        "            NSP_label = True if random.random() < 0.5 else False\n",
        "\n",
        "            if NSP_label:\n",
        "                sen1, sen2 = self.positive_sampling()\n",
        "                \n",
        "            else:\n",
        "                sen1, sen2 = self.negative_sampling()\n",
        "\n",
        "            if len(sen1) + len(sen2) > self.max_len-3:\n",
        "                sen1 = sen1[:(self.max_len-3)//2]\n",
        "                sen2 = sen2[:(self.max_len-3)//2]\n",
        "                \n",
        "            source_sentences = [self.CLS] + sen1 + [self.SEP] + sen2 + [self.SEP]\n",
        "            MLM_sentences, MLM_mask = self.masking(sen1,sen2)\n",
        "            \n",
        "            attention_mask = [1]*len(source_sentences)\n",
        "            token_type_ids = [0]*(len(sen1)+2) + [1]*(len(sen2)+1)\n",
        "            \n",
        "            # Zero padding\n",
        "            if len(source_sentences) < self.max_len:\n",
        "                num_pad = self.max_len - len(source_sentences)\n",
        "                \n",
        "                source_sentences = source_sentences + [0]*num_pad\n",
        "                MLM_sentences = MLM_sentences + [0]*num_pad\n",
        "                MLM_mask = MLM_mask + [0]*num_pad\n",
        "                attention_mask = attention_mask + [0]*num_pad \n",
        "                token_type_ids = token_type_ids + [0]*num_pad \n",
        "\n",
        "            \n",
        "            assert len(source_sentences) == len(MLM_sentences) == len(MLM_mask)\n",
        "            yield source_sentences, MLM_sentences, attention_mask, token_type_ids, MLM_mask, NSP_label\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fA-FR5watbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset = PretrainDataset(indexed_dataset, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LKervdWatbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_generator(trainset.__iter__, (tf.int32, tf.int32, tf.uint8, tf.int32, tf.uint8, tf.uint8))\n",
        "\n",
        "batch_size = 32\n",
        "dataset = dataset.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EXSWrKMatbP",
        "colab_type": "code",
        "colab": {},
        "outputId": "d3d6564c-654f-4caa-90de-e835bdead4f8"
      },
      "source": [
        "config = BertConfig()\n",
        "model = TFBertMainLayer(config)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "losses = []\n",
        "max_step = 100000\n",
        "for step, (source_sentences, MLM_sentences, attention_mask, token_type_ids, MLM_mask, NSP_label) in enumerate(dataset):\n",
        "    if step >= max_step:\n",
        "        break\n",
        "    #print(source_sentences, MLM_sentences, attention_mask, token_type_ids, MLM_mask, NSP_label)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        output = model(MLM_sentences, attention_mask, token_type_ids, training=True)\n",
        "        MLM_loss = compute_MLM_loss(source_sentences, MLM_mask, output[0])\n",
        "        #print(MLM_loss.shape)\n",
        "        #print(MLM_loss)\n",
        "        NSP_loss = compute_NSP_loss(labels=NSP_label, logits=output[1])\n",
        "        losses.append([MLM_loss, NSP_loss])\n",
        "        loss_value = MLM_loss + NSP_loss\n",
        "\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print('original sentence', np.array(vocab)[source_sentences[0]])\n",
        "        print('masked sentence', np.array(vocab)[MLM_sentences[0]])\n",
        "        print('mask', MLM_mask[0])\n",
        "        print('predicted sentence', np.array(vocab)[tf.keras.backend.argmax(output[0], axis=-1)[0]])\n",
        "\n",
        "        print(\n",
        "            \"Training loss (for one batch) at step %d: %.4f\"\n",
        "            % (step, float(loss_value))\n",
        "        )\n",
        "        print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[  101  1045  2245 ...     0     0     0]\n",
            " [  101  2012  1037 ...     0     0     0]\n",
            " [  101  2035 29624 ...     0     0     0]\n",
            " ...\n",
            " [  101  2138  1997 ...     0     0     0]\n",
            " [  101  2009  2987 ...     0     0     0]\n",
            " [  101  2026  3694 ...     0     0     0]], shape=(10, 128), dtype=int16) tf.Tensor(\n",
            "[[  101  1045  2245 ...     0     0     0]\n",
            " [  101  2012  1037 ...     0     0     0]\n",
            " [  101  2035 29624 ...     0     0     0]\n",
            " ...\n",
            " [  101  2138  1997 ...     0     0     0]\n",
            " [  101  2009  2987 ...     0     0     0]\n",
            " [  101   103  3694 ...     0     0     0]], shape=(10, 128), dtype=int16) tf.Tensor(\n",
            "[[ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]\n",
            " ...\n",
            " [ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]], shape=(10, 128), dtype=bool) tf.Tensor(\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]], shape=(10, 128), dtype=int16) tf.Tensor(\n",
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False  True False ... False False False]], shape=(10, 128), dtype=bool) tf.Tensor([ True  True False  True  True False False  True False False], shape=(10,), dtype=bool)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH2IbwBJatbR",
        "colab_type": "code",
        "colab": {},
        "outputId": "9c5c9480-20cf-4e9c-958d-0072f3536560"
      },
      "source": [
        "token_type_ids[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=51, shape=(128,), dtype=int16, numpy=\n",
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int16)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nIhmjIaatbT",
        "colab_type": "code",
        "colab": {},
        "outputId": "c07c8149-94af-4b63-96c9-1ec02f337c8e"
      },
      "source": [
        "source_sentences[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=55, shape=(128,), dtype=int16, numpy=\n",
              "array([  101,  2012,  1037,  2051,  2043,  2034, 29624, 27576, 28310,\n",
              "        2066, 27785,  3523,  5196,  1998,  4895, 22852,  2977,  2024,\n",
              "       18661,  2075,  2035,  1996,  3086,  1997,  3274, 27911,  2015,\n",
              "       29623,  8425,  7357,  2024,  1037,  5996,  8843, 29625,   102,\n",
              "        2007,  2307, 26136,  1998, 17211, 29623,  1996,  8364,  1997,\n",
              "       10608,  2479,  2003,  1037,  2208,  2008,  2111,  1997,  2035,\n",
              "        2287,  2967,  2052,  5959, 29625,   102,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0], dtype=int16)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgvfACU4atbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoding_index = [vocab[i] for i in source_sentences[1] if i!=0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa12_lDIatbX",
        "colab_type": "code",
        "colab": {},
        "outputId": "280ec62e-d1e9-467e-95d3-40a3849648eb"
      },
      "source": [
        "decoding_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'at',\n",
              " 'a',\n",
              " 'time',\n",
              " 'when',\n",
              " 'first',\n",
              " '##-',\n",
              " '##person',\n",
              " 'shooters',\n",
              " 'like',\n",
              " 'quake',\n",
              " 'iii',\n",
              " 'arena',\n",
              " 'and',\n",
              " 'un',\n",
              " '##real',\n",
              " 'tournament',\n",
              " 'are',\n",
              " 'garner',\n",
              " '##ing',\n",
              " 'all',\n",
              " 'the',\n",
              " 'attention',\n",
              " 'of',\n",
              " 'computer',\n",
              " 'gamer',\n",
              " '##s',\n",
              " '##,',\n",
              " 'graphic',\n",
              " 'adventures',\n",
              " 'are',\n",
              " 'a',\n",
              " 'dying',\n",
              " 'breed',\n",
              " '##.',\n",
              " '[SEP]',\n",
              " 'with',\n",
              " 'great',\n",
              " 'pun',\n",
              " 'and',\n",
              " 'humour',\n",
              " '##,',\n",
              " 'the',\n",
              " 'curse',\n",
              " 'of',\n",
              " 'monkey',\n",
              " 'island',\n",
              " 'is',\n",
              " 'a',\n",
              " 'game',\n",
              " 'that',\n",
              " 'people',\n",
              " 'of',\n",
              " 'all',\n",
              " 'age',\n",
              " 'groups',\n",
              " 'would',\n",
              " 'enjoy',\n",
              " '##.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6GVgFyatbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}